{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from clean_sentence import expand_contractions, remove_special, lemate_tokenize, stem_tokenize, split_sentences\n",
    "\n",
    "test[\"split_sentence\"] = test[\"sentence\"].apply(split_sentences)\n",
    "test = test.explode(\"split_sentences\")\n",
    "\n",
    "test[\"clean_sentence\"] = test[\"sentence\"].apply(expand_contractions)\n",
    "test[\"clean_sentence\"] = test[\"clean_sentence\"].apply(remove_special)\n",
    "test[\"words\"] = test[\"clean_sentence\"].apply(stem_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"clean_sentence\"] = train[\"sentence\"].apply(expand_contractions)\n",
    "train[\"clean_sentence\"] = train[\"clean_sentence\"].apply(remove_special)\n",
    "train[\"words\"] = train[\"clean_sentence\"].apply(stem_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the train_CNN script train a CNN on the training data, then label the testing data and filter out the data by keeping the sentences where the trained CNN agrees with the label.\n",
    "This ensures that the data is split by conjuctions having only 1 topic and they are correctly labled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def save_model(model, location):\n",
    "    file = open(location, 'wb')\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "def load_model(location):\n",
    "    file = open(location, 'rb')\n",
    "    model = pickle.load(file)\n",
    "    return model\n",
    "\n",
    "#Trained in train_word2vec script\n",
    "word2vec = load_model(\"word2vec.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# +1 for unknown vocab at the beginning\n",
    "embedding_matrix = np.zeros((len(word2vec.wv.vocab)+1, word2vec.vector_size))\n",
    "for i in range(len(word2vec.wv.vocab)):\n",
    "    embedding_vector = word2vec.wv[word2vec.wv.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i+1] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Reshape, SpatialDropout1D, Bidirectional, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, Lambda, TimeDistributed, Multiply, LSTM, RepeatVector, Permute, Activation, MaxPooling1D, AveragePooling1D, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_model(num_classes):\n",
    "  wv_layer = Embedding(embedding_matrix.shape[0],\n",
    "                      embedding_matrix.shape[1],\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=200,\n",
    "                      trainable=True)\n",
    "\n",
    "  # Inputs\n",
    "  comment_input = Input(shape=(200,))\n",
    "  mask_input = Input(shape=(200,))\n",
    "  mask_input_r = Reshape((200, 1))(mask_input)\n",
    "\n",
    "  embedded_sequences = wv_layer(comment_input)\n",
    "  embedded_sequences = SpatialDropout1D(0.5)(embedded_sequences)\n",
    "\n",
    "  embedded_sequences = concatenate([embedded_sequences, mask_input_r], axis=-1)\n",
    "\n",
    "  activations = Conv1D(128, 7, padding='same', activation='relu')(embedded_sequences)\n",
    "  #activations = LSTM(128, return_sequences=True)(embedded_sequences)\n",
    "  activations = SpatialDropout1D(0.5)(activations)\n",
    "  activations = BatchNormalization()(activations)\n",
    "  activations = AveragePooling1D(2)(activations)\n",
    "\n",
    "  activations = Conv1D(256, 7, padding='same', activation='relu')(activations)\n",
    "  #activations = LSTM(64, return_sequences=True)(embedded_sequences)\n",
    "  activations = SpatialDropout1D(0.5)(activations)\n",
    "  activations = BatchNormalization()(activations)\n",
    "  activations = AveragePooling1D(2)(activations)\n",
    "\n",
    "  activations = Conv1D(512, 7, padding='same', activation='relu')(activations)\n",
    "  #activations = LSTM(32, return_sequences=True)(embedded_sequences)\n",
    "  activations = SpatialDropout1D(0.5)(activations)\n",
    "  activations = BatchNormalization()(activations)\n",
    "  x = GlobalAveragePooling1D()(activations)\n",
    "\n",
    "  x = Dense(256, activation='relu')(x)\n",
    "  x = Dropout(0.5)(x)\n",
    "  x = BatchNormalization()(x)\n",
    "    \n",
    "  # outputs = []\n",
    "  # losses = []\n",
    "  # for i in range(num_classes):\n",
    "  #   outputs.append(Dense(1, activation='sigmoid')(x))\n",
    "  #   losses.append('binary_crossentropy')\n",
    "\n",
    "  outputs = Dense(num_classes, activation='softmax')(x)\n",
    "  losses = 'categorical_crossentropy'\n",
    "\n",
    "  # build the model\n",
    "  model = Model(inputs=[comment_input, mask_input], outputs=outputs)\n",
    "\n",
    "  model.compile(loss=losses, optimizer='adam')\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "from clean_sentence import TOPICS\n",
    "\n",
    "# This generator constructs new sentences by taking old ones and concatenate them together with conjunctions.\n",
    "# creates masks for important words (aka. topics)\n",
    "def generate_data(data, batch_size, max_old_size, num_classes):\n",
    "  \"\"\"Replaces Keras' native ImageDataGenerator.\"\"\"\n",
    "  data = data.reset_index(drop=True)\n",
    "  old_concats = pd.DataFrame(columns=[\"x\", \"mask\", \"y\"])\n",
    "  while True:\n",
    "    new_concats = pd.DataFrame(columns=[\"x\", \"mask\", \"y\"])\n",
    "\n",
    "    num_old = random.randint(0, batch_size)\n",
    "    if num_old < len(old_concats):\n",
    "      from_old = old_concats.sample(num_old)\n",
    "      old_concats = old_concats.drop(from_old.index, axis=0)\n",
    "      new_concats = pd.concat([new_concats, from_old])\n",
    "\n",
    "    while len(new_concats) < batch_size:\n",
    "      num_sent = random.randint(1, 3)\n",
    "      chosen = data.sample(num_sent)\n",
    "      X = []\n",
    "      Y = []\n",
    "      for j in range(num_sent):\n",
    "        X = X + chosen[\"words\"].iloc[j]\n",
    "        Y = Y + [chosen[\"labels\"].iloc[j]] * len(chosen[\"words\"].iloc[j])\n",
    "        if (j < num_sent-1) and random.randint(0, 5)>0:\n",
    "          X = X + [random.choice([\"and\", \"or\", \"but\", \"howev\", \"although\", \"moreov\", 'also', 'further', 'furthermor', 'so'])]\n",
    "          Y = Y + [-1]\n",
    "\n",
    "      added = False\n",
    "      for i in range(len(X)):\n",
    "        if X[i] in TOPICS:\n",
    "          mask = [0]*len(X)\n",
    "          mask[i] = 1\n",
    "          if added:\n",
    "            old_concats = old_concats.append({'x': X, 'mask': mask, 'y': Y[i]}, ignore_index=True)\n",
    "            if len(old_concats) > max_old_size:\n",
    "              old_concats = old_concats.drop(random.choice(old_concats.index), axis=0)\n",
    "          else:\n",
    "            added = True\n",
    "            new_concats = new_concats.append({'x': X, 'mask': mask, 'y': Y[i]}, ignore_index=True)\n",
    "    \n",
    "    x_data = [[word2vec.wv.vocab[t].index +1 if t in word2vec.wv.vocab else 0 for t in comment] for comment in new_concats['x'].tolist()]\n",
    "    x_data = pad_sequences(x_data, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "    y_data = np.eye(num_classes)[new_concats['y'].tolist()]\n",
    "    mask_data = pad_sequences(new_concats['mask'].tolist(), maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "    yield [[x_data, mask_data], y_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, r2_score, f1_score, confusion_matrix\n",
    "from sklearn.utils import shuffle, class_weight\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "print('**Training Model')\n",
    "\n",
    "batch_size = 128\n",
    "model = get_model(5)\n",
    "model.fit_generator(\n",
    "    generate_data(train, batch_size, 1000, 3),\n",
    "    epochs=100,\n",
    "    steps_per_epoch = 200,\n",
    "    validation_data = generate_data(test, batch_size, 1000, 3),\n",
    "    validation_steps = 2\n",
    "    )\n",
    "model.save_weights(\"keras_topic_sentiment.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Fantastic car, although has one problem with a rattling sound coming from the steering column.\"\n",
    "sent = tokenize(remove_special(expand_contractions(sent)))\n",
    "x_data = [[word2vec.wv.vocab[w].index +1 for w in sent if w in word2vec.wv.vocab]]\n",
    "x_data = pad_sequences(x_data, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "mask = [0]*len(sent)\n",
    "mask[sent.index('steer')] = 1\n",
    "mask_data = pad_sequences([mask], maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "model.predict([x_data, mask_data]).tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
