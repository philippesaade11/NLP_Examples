{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from clean_sentence import expand_contractions, remove_special, lemate_tokenize, stem_tokenize\n",
    "\n",
    "data[\"clean_sentence\"] = data[\"sentence\"].apply(expand_contractions)\n",
    "data[\"clean_sentence\"] = data[\"clean_sentence\"].apply(remove_special)\n",
    "data[\"words\"] = data[\"clean_sentence\"].apply(stem_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def save_model(model, location):\n",
    "    file = open(location, 'wb')\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "def load_model(location):\n",
    "    file = open(location, 'rb')\n",
    "    model = pickle.load(file)\n",
    "    return model\n",
    "\n",
    "#Trained in train_word2vec script\n",
    "word2vec = load_model(\"word2vec.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# +1 for unknown vocab at the beginning\n",
    "embedding_matrix = np.zeros((len(word2vec.wv.vocab)+1, word2vec.vector_size))\n",
    "for i in range(len(word2vec.wv.vocab)):\n",
    "    embedding_vector = word2vec.wv[word2vec.wv.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i+1] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "X_vocab = [[word2vec.wv.vocab[t].index +1 if t in word2vec.wv.vocab else 0 for t in comment] for comment in data[\"words\"].tolist()]\n",
    "X_vocab = pad_sequences(X_vocab, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Embedding, Dropout, SpatialDropout1D, Conv1D, GlobalAveragePooling1D, Lambda, AveragePooling1D, concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    false_positive_weight = 0.2\n",
    "    thresh = 0.5\n",
    "    y_pred_true = K.greater_equal(thresh, y_pred)\n",
    "    y_not_true = K.less_equal(thresh, y_true)\n",
    "    \n",
    "    false_positive_tensor = K.equal(y_pred_true, y_not_true)\n",
    "    false_positive_tensor = K.cast(false_positive_tensor, 'float32')\n",
    "    \n",
    "    complement = 1 - false_positive_tensor\n",
    "    falsePosGroupTrue = y_true * false_positive_tensor\n",
    "    falsePosGroupPred = y_pred * false_positive_tensor\n",
    "    \n",
    "    nonFalseGroupTrue = y_true * complement\n",
    "    nonFalseGroupPred = y_pred * complement\n",
    "    \n",
    "    falsePosLoss = K.binary_crossentropy(falsePosGroupTrue, falsePosGroupPred)\n",
    "    nonFalseLoss = K.binary_crossentropy(nonFalseGroupTrue, nonFalseGroupPred)\n",
    "    \n",
    "    return (false_positive_weight * falsePosLoss) + nonFalseLoss\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return (1 - K.mean(f1)) + K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "def get_model(num_classes):\n",
    "  wv_layer = Embedding(embedding_matrix.shape[0],\n",
    "                      embedding_matrix.shape[1],\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=200,\n",
    "                      trainable=True)\n",
    "\n",
    "  # Inputs\n",
    "  comment_input = Input(shape=(200,))\n",
    "  embedded_sequences = wv_layer(comment_input)\n",
    "\n",
    "  # biGRU\n",
    "  embedded_sequences = SpatialDropout1D(0.5)(embedded_sequences)\n",
    "\n",
    "  activations = Conv1D(128, 7, padding='same', activation='relu')(embedded_sequences)\n",
    "  #activations = LSTM(128, return_sequences=True)(embedded_sequences)\n",
    "  activations = SpatialDropout1D(0.5)(activations)\n",
    "  activations = BatchNormalization()(activations)\n",
    "  activations = AveragePooling1D(2)(activations)\n",
    "\n",
    "  activations = Conv1D(256, 7, padding='same', activation='relu')(activations)\n",
    "  #activations = LSTM(64, return_sequences=True)(embedded_sequences)\n",
    "  activations = SpatialDropout1D(0.5)(activations)\n",
    "  activations = BatchNormalization()(activations)\n",
    "  activations = AveragePooling1D(2)(activations)\n",
    "\n",
    "  activations = Conv1D(512, 7, padding='same', activation='relu')(activations)\n",
    "  #activations = LSTM(32, return_sequences=True)(embedded_sequences)\n",
    "  activations = SpatialDropout1D(0.5)(activations)\n",
    "  activations = BatchNormalization()(activations)\n",
    "  x = GlobalAveragePooling1D()(activations)\n",
    "\n",
    "  x = Dense(256, activation='relu')(x)\n",
    "  x = Dropout(0.5)(x)\n",
    "  x = BatchNormalization()(x)\n",
    "    \n",
    "  outputs = []\n",
    "  losses = []\n",
    "  for i in range(num_classes):\n",
    "    outputs.append(Dense(1, activation='sigmoid')(x))\n",
    "    losses.append('binary_crossentropy')\n",
    "\n",
    "  # build the model\n",
    "  model = Model(inputs=[comment_input], outputs=outputs)\n",
    "\n",
    "  model.compile(loss=losses, optimizer='adam')\n",
    "  #model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = (data[\"split\"] == \"train\")\n",
    "test = (data[\"split\"] == \"test\")\n",
    "validate = (data[\"split\"] == \"validate\")\n",
    "\n",
    "categories = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "\n",
    "y_train = pd.get_dummies(data[train][\"sentiment\"])\n",
    "x_train = X_vocab[train]\n",
    "\n",
    "y_test = pd.get_dummies(data[test][\"sentiment\"])\n",
    "x_test = X_vocab[test]\n",
    "\n",
    "y_train = y_train[categories]\n",
    "y_test = y_test[categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate and Train Model\n",
    "\n",
    "from sklearn.metrics import accuracy_score, r2_score, f1_score, confusion_matrix\n",
    "from sklearn.utils import shuffle, class_weight\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "print('**Training Model')\n",
    "x_train, y_train = shuffle(x_train, y_train)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', y_train.columns, y_train.idxmax(axis=1))\n",
    "class_weights = dict(zip(list(range(len(categories))), class_weights))\n",
    "\n",
    "model = get_model(len(categories))\n",
    "model.fit([x_train], y_train.to_numpy().T.tolist(), epochs=40, batch_size=512, validation_data=([x_test], y_test.to_numpy().T.tolist()), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate Model\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "y_pred = np.array(model.predict([x_test])).T[0]\n",
    "#y_pred = pd.DataFrame((y_pred > 0.5).reshape(len(y_test), 5), columns=categories).astype(int) #Take any value above 0.5\n",
    "y_pred = pd.DataFrame(pd.get_dummies(np.argmax(y_pred, axis=1)).values, columns=categories) #Take largest value\n",
    "\n",
    "for category in categories:\n",
    "    print(\"{}**\".format(category))\n",
    "    print(confusion_matrix(y_test[category].tolist(), y_pred[category].tolist()))\n",
    "    print(accuracy_score(y_test[category].tolist(), y_pred[category].tolist()))\n",
    "    print(f1_score(y_test[category].tolist(), y_pred[category].tolist()))\n",
    "\n",
    "print(\"\\nTotal\")\n",
    "print(confusion_matrix(y_test[categories].idxmax(axis=1), y_pred[categories].idxmax(axis=1)))\n",
    "print(accuracy_score(y_test[categories].idxmax(axis=1), y_pred[categories].idxmax(axis=1)))\n",
    "print(f1_score(y_test[categories].idxmax(axis=1), y_pred[categories].idxmax(axis=1), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Model on new dataset\n",
    "\n",
    "newdata = pd.read_excel(\"data\")\n",
    "data[\"clean_sentence\"] = data[\"sentence\"].apply(expand_contractions)\n",
    "data[\"clean_sentence\"] = data[\"clean_sentence\"].apply(remove_special)\n",
    "data[\"words\"] = data[\"clean_sentence\"].apply(stem_tokenize)\n",
    "    \n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "X_vocab = [[word2vec.wv.vocab[t].index +1 if t in word2vec.wv.vocab else 0 for t in comment] for comment in data[\"words\"].tolist()]\n",
    "X_vocab = pad_sequences(X_vocab, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "predictions = pd.DataFrame(np.array(model.predict(X_vocab)).T[0]\n",
    "data = pd.concat([data.reset_index(), predictions, columns=categories)], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
