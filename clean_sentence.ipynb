{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import re\n",
    "\n",
    "CONTRACTION_MAP = {\n",
    "\t\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\", \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "#for example replace doesn't with do not\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    text = unidecode(text)\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove special characters, extra spacing...\n",
    "def remove_special(text):\n",
    "    text = re.sub(r\"([A-Z])([a-z])\", r\" \\1\\2\", text)\n",
    "    text = re.sub(\"[^'\\\"a-zA-Z0-9\\.!\\?\\-_, ]+\", \"\", text.lower())\n",
    "    text = re.sub(\"[\\.!\\?\\-_, ]+\", \" \", text)\n",
    "    text = re.sub(\"[\\s]+\", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize and Remove Stop Words\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#Lemmate and add pos tag to the word, and tokenize\n",
    "def lemate_tokenize(text):\n",
    "  word_tokens = word_tokenize(text)\n",
    "  word_tagged = nltk.pos_tag(word_tokens)\n",
    "  \n",
    "  word_lemmatized = []\n",
    "  for word, tag in word_tagged:\n",
    "    if(tag[0] in [\"V\", \"N\", \"J\", \"R\"]): #keep only verbs, nouns and adjectives\n",
    "      wntag = get_wordnet_pos(tag)\n",
    "      if wntag is None: # not supply tag in case of None\n",
    "          word_lemmatized.append(lemmatizer.lemmatize(word)+\"_\"+tag[0])\n",
    "      else:\n",
    "          word_lemmatized.append(lemmatizer.lemmatize(word, pos=wntag)+\"_\"+tag[0])\n",
    "\n",
    "  # Remove stop words\n",
    "  filtered_sentence = [w for w in word_lemmatized if w not in stop_words]\n",
    "\n",
    "  return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "#Stem then tokenize\n",
    "def stem_tokenize(text):\n",
    "  word_tokens = word_tokenize(text)\n",
    "  word_tokens = [sno.stem(w) for w in word_tokens]\n",
    "  return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    og_text = text\n",
    "    try:\n",
    "            # hi (hello there) you    ->    hi, hello there, you\n",
    "        text = re.sub(r\"\\s*\\(([a-z])\\)\", r\" \\1)\", text)\n",
    "        text = re.sub(r'\\s*\\((.*?)\\)', r', \\1,', text)\n",
    "        text = re.sub(r'\\s*\\[(.*?)\\]', r', \\1,', text)\n",
    "        text = re.sub(r'\\s*{(.*?)}', r', \\1,', text)\n",
    "\n",
    "            # split \"categories: 1) first. 2) second.\"\n",
    "        text = re.sub(r\"((^|\\s+)[1-9a-zA-Z]\\s*(\\)|-|(\\.\\s)))\", r\" <split><conj>\\1<conj><split> \", text)\n",
    "        text = re.sub(r\"([:;])\", r\" <split><conj>\\1<conj><split> \", text)\n",
    "            \n",
    "            # split \"abc -- cde\", split new lines, remove extra spaces\n",
    "        text = re.sub(r\"(\\s+-+\\s+)\", r\" <split><conj>\\1<conj><split> \", text)\n",
    "        text = re.sub(\"[\\n]+\", \". \", text)\n",
    "        text = re.sub(\"(\\s?\\.)+\", \".\", text)\n",
    "        text = re.sub(\"[\\s]+\", \" \", text)\n",
    "            \n",
    "            # split at conjunctions\n",
    "        for conj in ['and', 'but', 'also', 'further', 'furthermore', 'moreover', 'so']:\n",
    "            text = re.sub(\"(\\s+\"+conj+\"\\s+)\", r\" <split><conj>\\1<conj><split> \", text, flags=re.I)\n",
    "\n",
    "            # split sentence with nltk. \"apndsent\" that are previously added to check if sentences split between \"apndsent\" are worth spliting by checking if they contain a verb and a noun.\n",
    "        text = text.split(\"<split>\")\n",
    "        text = [nltk.sent_tokenize(t) if \"<conj>\" not in t else [t] for t in text]\n",
    "        text = [t for tt in text for t in tt]\n",
    "        text = [re.sub(\"[.?!,:;]+$\", \"\", s.strip()) for s in text]\n",
    "        text = [re.sub(\"^[.?!,:;]+\", \"\", s.strip()) for s in text if s != '']\n",
    "        \n",
    "        if len(text) == 0:\n",
    "            return \"\"\n",
    "\n",
    "            # stick sentences back together if they are short.\n",
    "        if \"<conj>\" in text[0]:\n",
    "            text = text[1:]\n",
    "        if \"<conj>\" in text[-1]:\n",
    "            text = text[:-1]\n",
    "        if len(text) > 1:\n",
    "            i=0\n",
    "            add_prev = (False, ', ')\n",
    "            while(i < len(text)):\n",
    "                if(\"<conj>\" in text[i]):\n",
    "                    add_prev = (True, re.findall(\"<conj>(.*)<conj>\", text[i])[0])\n",
    "                    text = text[:i]+text[i+1:]\n",
    "                else:\n",
    "    #                 poz = nlp(text[i])\n",
    "    #                 poz = [w.pos_ for w in poz]\n",
    "    #                 is_sent = (\"AUX\" in poz or \"VERB\" in poz) and (\"NOUN\" in poz or \"PRON\" in poz or \"PROPN\" in poz)\n",
    "                    is_sent = len(text[i].split(\" \")) >= 5\n",
    "                    \n",
    "                    if not is_sent:\n",
    "                        if add_prev[0] or i == len(text)-1:\n",
    "                            text = text[:i-1] + [text[i-1]+ add_prev[1] +text[i]] + text[i+1:]\n",
    "                        else:\n",
    "                            while \"<conj>\" in text[i+1]:\n",
    "                                add_prev = (False, re.findall(\"<conj>(.*)<conj>\", text[i+1])[0])\n",
    "                                text = text[:i+1]+text[i+2:]\n",
    "                            text = text[:i] + [text[i]+ add_prev[1] +text[i+1]] + text[i+2:]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                    add_prev = (False, ', ')\n",
    "                    \n",
    "        text = [re.sub(\"[\\s\\.]*<conj>[\\s\\.]*\", \" \", s).strip() for s in text]\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        print(og_text)\n",
    "        print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import math\n",
    "from unidecode import unidecode\n",
    "\n",
    "#List of words that are considered important, aka. topics (after they get stemmed)\n",
    "TOPICS = [\"seat\", \"seatbelt\", \"carpet\", \"headrest\", \"chair\", \"backrest\", \"backseat\", \"window\", \"engin\", \"gearbox\", \"bluetooth\", \"audio\", \"music\", \"cuphold\", \"cup\", \"storag\", \"armrest\",\n",
    " \"space\", \"luggag\", \"paint\", \"paintwork\", \"bumper\", \"wheel\", \"rim\", \"tyre\", \"rack\", \"design\", \"style\", \"shape\", \"aesthet\", \"layout\", \"color\", \"colour\", \"devic\", \"car\", \"vehicl\",\n",
    " \"motorcycl\", \"motorbik\", \"bmw\", \"bike\", \"competitor\", \"product\", \"build\", \"brand\", \"light\", \"sensor\", \"headlight\", \"lamp\", \"alarm\", \"wiper\", \"windshield\", \"brake\", \"break\", \"horn\",\n",
    " \"handbrak\", \"it\", \"dealer\", \"dealership\", \"deaelrship\", \"seller\", \"salesman\", \"salesperson\", \"salesmen\", \"consult\", \"mirror\", \"camera\", \"feature\", \"gadget\", \"configur\", \"model\",\n",
    " \"radar\", \"trunk\", \"hatch\", \"seri\", \"sporti\", \"countryman\", \"materi\", \"fabric\", \"interior\", \"leather\", \"exterior\", \"ambient\", \"usb\", \"aux\", \"port\", \"cd\", \"it\", \"system\", \"gps\",\n",
    " \"function\", \"interfac\", \"touchscreen\", \"softwar\", \"featur\", \"comput\", \"program\", \"economi\", \"econom\", \"handl\", \"handel\"]\n",
    "\n",
    "def split_sentences_2(text, threshold=10, non_nltk_penalty=0.75):\n",
    "    text = sentence_extract_conj(text)\n",
    "    return sentence_split_tree(text, TOPICS, non_nltk_penalty=non_nltk_penalty, threshold=threshold)\n",
    "\n",
    "def sentence_extract_conj(text):\n",
    "    text = unidecode(text)\n",
    "        # hi (hello there) you    ->    hi, hello there, you\n",
    "    text = re.sub(r\"\\s*\\(([a-z])\\)\", r\" \\1)\", text)\n",
    "    text = re.sub(r'\\s*\\((.*?)\\)', r', \\1,', text)\n",
    "    text = re.sub(r'\\s*\\[(.*?)\\]', r', \\1,', text)\n",
    "    text = re.sub(r'\\s*{(.*?)}', r', \\1,', text)\n",
    "        \n",
    "        # split \"abc -- cde\", split new lines, remove extra spaces\n",
    "    text = re.sub(\"[\\n]+\", \". \", text)\n",
    "    text = re.sub(\"(\\s?\\.)+\", \".\", text)\n",
    "    text = re.sub(r\"(\\s+-+\\s+)\", r\" <conj>\\1<conj> \", text)\n",
    "        \n",
    "        # split \"categories: 1) first. 2) second.\"\n",
    "    text = re.sub(r\"((^|\\s+)[1-9a-zA-Z]\\s*(\\)|-))\", r\" <conj>\\1<conj> \", text)\n",
    "    text = re.sub(r\"([:;])\", r\" <conj>\\1<conj> \", text)\n",
    "\n",
    "        # split at conjunctions\n",
    "    for conj in ['and', 'but', 'also', 'further', 'furthermore', 'moreover', 'so']:\n",
    "        text = re.sub(\"(\\s+\"+conj+\"\\s+)\", r\" <conj>\\1<conj> \", text, flags=re.I)\n",
    "\n",
    "        # split sentence with nltk. \"<split><conj>\" that are previously added to check if sentences split between \"<split><conj>\" are worth spliting by checking if they contain a topic.\n",
    "    text = \"<conj><nltk><conj>\".join(nltk.sent_tokenize(text))\n",
    "    return text\n",
    "\n",
    "def sentence_split_tree(text, topics, threshold=10, non_nltk_penalty=0.75):\n",
    "    tree = sent_tree(text)\n",
    "    tree.construct(topic_scorer(topics, non_nltk_penalty=non_nltk_penalty), threshold=threshold)\n",
    "    text = tree.get_splits()\n",
    "    text = [re.sub(\"[\\s\\.]*(<conj>)|(<nltk>)[\\s\\.]*\", \" \", s).strip() for s in text]\n",
    "    text = [re.sub(\"[\\s]+\", \" \", s).strip() for s in text]\n",
    "    text = [re.sub(\"[.?!,:;]+$\", \"\", s.strip()) for s in text]\n",
    "    text = [re.sub(\"^[.?!,:;]+\", \"\", s.strip()) for s in text if s != '']\n",
    "    return text\n",
    "\n",
    "class sent_tree:\n",
    "    text = \"\"\n",
    "    left_child = None\n",
    "    right_child = None\n",
    "\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "    def construct(self, scorer, threshold=10):\n",
    "        p = re.compile(\"<conj>((?!<conj>).)*<conj>\")\n",
    "        max_score = -1\n",
    "        position = None\n",
    "        for conj in p.finditer(self.text):\n",
    "            score = scorer(self.text, conj.start(), conj.end())\n",
    "            if(score > max_score):\n",
    "                max_score = score\n",
    "                position = (conj.start(), conj.end())\n",
    "\n",
    "        # self.text = self.text +\" \"+ str(max_score)\n",
    "        if(max_score > threshold):\n",
    "            self.left_child = sent_tree(self.text[:position[0]])\n",
    "            self.right_child = sent_tree(self.text[position[1]:])\n",
    "            self.text = self.text[position[0]:position[1]]\n",
    "            self.left_child.construct(scorer)\n",
    "            self.right_child.construct(scorer)\n",
    "\n",
    "    def get_splits(self):\n",
    "        if(self.left_child is None and self.right_child is None):\n",
    "            return [self.text]\n",
    "        lpart = self.left_child.get_splits() if self.left_child is not None else []\n",
    "        rpart = self.right_child.get_splits() if self.right_child is not None else []\n",
    "        return lpart + rpart\n",
    "    \n",
    "def topic_scorer(topics, non_nltk_penalty=0.75):\n",
    "    def entropy(val1, val2):\n",
    "        if (val1+val2) == 0:\n",
    "            return 0 \n",
    "        lprob = val1 / (val1 + val2)\n",
    "        rprob = val2 / (val1 + val2)\n",
    "        return -(lprob*math.log(lprob, 2) if lprob!=0 else 0) - (rprob*math.log(rprob, 2) if rprob!=0 else 0)\n",
    "    \n",
    "    def score_func(text, start, end):\n",
    "        lpart = re.sub(\"[\\s\\.]*(<conj>)|(<nltk>)[\\s\\.]*\", \" \", text[:start]).strip()\n",
    "        rpart = re.sub(\"[\\s\\.]*(<conj>)|(<nltk>)[\\s\\.]*\", \" \", text[end:]).strip()\n",
    "        \n",
    "        lsum = sum([x in topics for x in nltk.word_tokenize(lpart)])\n",
    "        rsum = sum([x in topics for x in nltk.word_tokenize(rpart)])\n",
    "        entval = entropy(float(lsum), float(rsum))\n",
    "        \n",
    "        conj_penalty = 1 if \"<nltk>\" in text[start:end] else non_nltk_penalty\n",
    "        conj_penalty = 0.25 if len(nltk.word_tokenize(lpart))<5 or len(nltk.word_tokenize(lpart))<5 else conj_penalty\n",
    "        return entval*(lsum+rsum)*conj_penalty\n",
    "    return score_func"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
